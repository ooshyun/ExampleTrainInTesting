이제 논문 **2601.16175v1 "Learning to Discover at Test Time"**의 내용을 정리해 드리겠습니다.

---

## 📄 논문 개요

**제목:** Learning to Discover at Test Time  
**저자:** Mert Yuksekgonul 외 다수 (Stanford University, NVIDIA, Astera Institute, UC San Diego, Together AI)  
**arXiv:** 2601.16175v1 (2025년 1월 22일)

---

## 🎯 핵심 아이디어

기존의 테스트 타임 스케일링 방법(예: AlphaEvolve)은 **고정된 LLM**을 프롬프팅하여 탐색을 수행합니다. 이 논문은 **테스트 타임에 강화학습(RL)을 수행**하여 LLM이 특정 테스트 문제에 대한 경험으로 계속 학습할 수 있도록 합니다.

이 방법을 **TTT-Discover (Test-Time Training to Discover)**라고 명명했습니다.

---

## 🔬 주요 특징

### 1. 일반 RL과의 차이점
- **목표:** 평균 보상 최대화가 아닌, **단 하나의 최고 솔루션** 발견
- **일반화 불필요:** 다른 문제로 일반화할 필요 없이 **현재 문제만 해결**
- 따라서 학습 목표와 탐색 서브루틴이 **가장 유망한 솔루션을 우선시**하도록 설계

### 2. 두 가지 핵심 구성요소

**① 엔트로픽 목표 함수 (Entropic Objective)**
- 기대 보상이 아닌 **최대 보상 행동을 선호**하는 목표 함수
- 적응형 온도 파라미터 ς(s)를 사용하여 안정적인 학습

**② PUCT 기반 상태 재사용**
- Monte Carlo Tree Search에서 영감받은 선택 규칙
- 평균이 아닌 **최대 보상**을 Q값으로 사용
- 탐색-활용 균형을 위한 탐색 보너스 포함

---

## 📊 실험 결과

| 분야 | 문제 | 최고 인간 | 이전 최고 AI | TTT-Discover |
|------|------|-----------|--------------|--------------|
| **수학** | Erdős 최소 겹침 문제 | 0.380927 | 0.380924 | **0.380876** ✨ |
| **수학** | 자기상관 부등식 1 | 1.50973 | 1.50317 | **1.50287** ✨ |
| **커널 공학** | TriMul (H100) | 1371 µs | N/A | **1161 µs** ✨ |
| **커널 공학** | TriMul (A100) | 4531 µs | N/A | **2198 µs** (50% 향상) ✨ |
| **알고리즘** | AtCoder ahc039 | 566,997 | 558,026 | **567,062** ✨ |
| **알고리즘** | AtCoder ahc058 | 847.6M | 848.4M | **848.4M** ✨ |
| **생물학** | 단일 세포 디노이징 | 0.64 | N/A | **0.71** ✨ |

---

## 🔧 구현 세부사항

- **모델:** gpt-oss-120b (오픈 모델)
- **플랫폼:** Tinker (Thinking Machines의 API)
- **학습:** 50 스텝, 스텝당 512 롤아웃
- **비용:** 문제당 약 $500
- **LoRA:** rank 32 사용

---

## 🏆 주요 성과

1. **Erdős 최소 겹침 문제:** AlphaEvolve 개선폭의 **16배** 큰 개선
2. **GPU 커널:** 모든 GPU 타입에서 **15% 이상 성능 향상**
3. **AtCoder 대회:** 두 대회 모두 **1위** 달성 (제출 시점 기준)
4. **오픈 모델만으로 달성:** 폐쇄형 프론티어 모델 없이 최고 성능

---

## 📈 Ablation Study 결과

| 구성 | 최고 런타임 (µs) |
|------|-----------------|
| **TTT-Discover (전체)** | **1161.2** |
| 상수 ς 사용 | 1483.8 |
| 엔트로픽 목표 없음 | 1985.7 |
| TTT 없음 (재사용만) | 2060.7 |
| ε-greedy 재사용 | 1328.9 |
| 재사용 없음 | 5274.0 |
| Best-of-N | 5352.4 |

→ **모든 구성요소가 필수적**임을 확인

---

## 💡 한계 및 향후 연구

- 현재는 **연속적 보상**이 있는 문제에만 적용 가능
- 향후: 희소/이진 보상 문제, 검증 불가능한 도메인으로 확장 필요

---

## 🔍 결론

TTT-Discover는 테스트 타임에 LLM을 계속 학습시켜 과학적 발견 문제에서 **새로운 최고 성능**을 달성하는 방법입니다. 핵심은 발견 문제의 특성(최대값 중심, 단일 문제 집중)에 맞춘 **엔트로픽 목표 함수**와 **PUCT 기반 상태 재사용**입니다.


테스트 타임 강화학습의 개념과 시스템 파이프라인을 자세히 설명해 드리겠습니다.

## 🤔 "테스트 타임에 RL을 수행한다"는 의미

### 기존 방식 vs TTT-Discover

**기존 방식 (AlphaEvolve 등):**
- 학습된 LLM의 가중치(θ)는 **고정**
- 프롬프트만 바꿔가며 여러 솔루션 생성
- 모델 자체는 절대 변하지 않음
- 마치 "똑같은 학생에게 문제만 다르게 내는 것"

**TTT-Discover:**
- 테스트 문제를 풀면서 LLM 가중치(θ)를 **실시간 업데이트**
- 시도할수록 모델이 **이 특정 문제에 더 능숙**해짐
- 마치 "학생이 문제를 풀면서 동시에 공부하는 것"

---

이제 시스템 파이프라인을 시각화해 드리겠습니다:위 다이어그램에서 핵심 내용을 텍스트로도 정리해 드리겠습니다.

---

## 🎯 "테스트 타임 RL"의 핵심 이해

### 일반적인 ML 파이프라인
```
[학습 데이터] → [모델 훈련] → [고정된 모델] → [테스트 시 예측만]
                              (θ 고정)
```

### TTT-Discover 파이프라인
```
[테스트 문제] → [시도] → [피드백] → [모델 업데이트] → [더 나은 시도] → ...
                                    (θ 계속 변화!)
```

---

## 📊 6단계 루프 (매 스텝마다 반복)

| 단계 | 동작 | 설명 |
|------|------|------|
| **1. 초기화** | 문제 d + 초기 모델 θ₀ | 테스트 문제와 사전학습 모델 준비 |
| **2. 상태 선택** | PUCT로 시작점 선택 | 버퍼에서 유망한 이전 솔루션 선택 |
| **3. 액션 생성** | LLM이 코드 생성 | 512개 솔루션 병렬 생성 |
| **4. 환경 실행** | 코드 실행 & 평가 | 실제 성능 측정 (런타임 등) |
| **5. 버퍼 저장** | 경험 축적 | (상태, 액션, 보상) 저장 |
| **⚡6. 모델 학습** | **θᵢ → θᵢ₊₁** | **이게 핵심! 모델 가중치 업데이트** |

---

## 💡 비유로 이해하기

**기존 방식 (AlphaEvolve):**
> 수학 시험에서 같은 실력인 학생이 문제를 25,600번 다시 푸는 것
> → 운 좋게 맞출 때까지 반복

**TTT-Discover:**
> 학생이 문제를 풀 때마다 **오답 노트**를 만들고 **공부**하면서 푸는 것
> → 50번 시도할 때마다 실력이 향상됨

---

## 📈 실제 효과 (Figure 1에서 확인 가능)

```
Step 0:   [낮은 보상 분포] ████████░░░░░░░░░░░░
Step 9:   [약간 개선]      ░░░████████░░░░░░░░░
Step 24:  [상당히 개선]    ░░░░░░░████████░░░░░
Step 49:  [SOTA 달성!]     ░░░░░░░░░░░░████████  ← 최고 성능
```

학습이 진행될수록 솔루션 품질 분포가 오른쪽(더 좋은 보상)으로 이동합니다. 이것이 기존 Best-of-N 방식과의 결정적 차이입니다.


RL 배경 없이도 이해할 수 있도록 차근차근 설명해 드릴게요!

## 🎓 먼저 핵심 개념부터

### LLM이란 무엇인가? (아주 단순화)

LLM은 본질적으로 **거대한 수학 함수**입니다:

```
입력(프롬프트) → [수십억 개의 숫자(가중치 θ)] → 출력(텍스트)
```

이 "숫자들(θ)"이 바뀌면 같은 입력에도 **다른 출력**이 나옵니다.

---

## 📚 RL 학습의 핵심 아이디어 (비유)

### 🐕 강아지 훈련으로 이해하기

```
강아지가 "앉아"를 배우는 과정:

1. 강아지가 행동함 (앉거나, 눕거나, 뛰거나...)
2. 앉으면 → 간식! (보상 👍)
   안 앉으면 → 간식 없음 (보상 없음 👎)
3. 강아지 뇌가 업데이트: "앉으면 좋은 일이 생기는구나"
4. 다음에 "앉아" 들으면 → 앉을 확률 UP ⬆️
```

**LLM도 똑같습니다:**

```
1. LLM이 코드를 생성함 (여러 가지 시도)
2. 빠른 커널 → 높은 보상! 👍
   느린 커널 → 낮은 보상 👎
3. LLM 가중치(θ)가 업데이트: "이런 패턴의 코드가 좋구나"
4. 다음에 비슷한 문제 → 좋은 코드 생성 확률 UP ⬆️
```

---

## 🔍 실제 TTT-Discover 학습 과정

### Step 1: 경험 데이터 수집

한 스텝에서 512개의 솔루션을 생성합니다:

```
솔루션 1: "def kernel_v1(): ..." → 실행시간 5000µs → 보상 0.2 (느림 😢)
솔루션 2: "def kernel_v2(): ..." → 실행시간 3000µs → 보상 0.33 (보통 😐)
솔루션 3: "def kernel_v3(): ..." → 실행시간 1500µs → 보상 0.67 (빠름! 😊)
...
솔루션 512: "def kernel_v512(): ..." → 실행시간 1200µs → 보상 0.83 (최고! 🎉)
```

이제 우리는 **"어떤 출력이 좋고 나쁜지"** 알게 됐습니다.

---

### Step 2: 가중치 업데이트 원리

핵심 질문: **"좋은 솔루션을 더 자주 생성하려면 θ를 어떻게 바꿔야 할까?"**

#### 직관적 설명:

```
현재 LLM 상태:
┌─────────────────────────────────────┐
│  "def kernel"을 보면...              │
│  → 패턴 A 생성 확률: 30%  (느린 코드) │
│  → 패턴 B 생성 확률: 50%  (보통 코드) │
│  → 패턴 C 생성 확률: 20%  (빠른 코드) │
└─────────────────────────────────────┘

우리가 원하는 것:
┌─────────────────────────────────────┐
│  "def kernel"을 보면...              │
│  → 패턴 A 생성 확률: 10% ⬇️         │
│  → 패턴 B 생성 확률: 30% ⬇️         │
│  → 패턴 C 생성 확률: 60% ⬆️⬆️      │
└─────────────────────────────────────┘
```

**어떻게?** → 가중치 θ를 조금씩 조정!

---

### Step 3: 수학적으로 (최대한 쉽게)

#### 기본 RL 공식:

```
θ_새로운 = θ_현재 + 학습률 × 방향
```

여기서 **"방향"**은 "좋은 출력을 낼 확률을 높이는 방향"입니다.

#### 구체적으로:

```python
# 의사 코드로 이해하기

for each (입력, 출력, 보상) in 경험데이터:
    
    if 보상이 높으면:
        # 이 출력을 낼 확률을 높여!
        θ를 조정하여 → P(이 출력 | 이 입력) ⬆️
    
    if 보상이 낮으면:
        # 이 출력을 낼 확률을 낮춰!
        θ를 조정하여 → P(이 출력 | 이 입력) ⬇️
```

---

## ⭐ TTT-Discover의 특별한 점: 엔트로픽 목표

### 일반 RL vs TTT-Discover

**일반 RL:**
```
"평균적으로 좋은 출력을 내자"

보상 [0.2, 0.3, 0.5, 0.8] → 평균 = 0.45
→ 모든 솔루션을 비슷하게 취급
```

**TTT-Discover (엔트로픽 목표):**
```
"최고의 출력에 집중하자!"

보상 [0.2, 0.3, 0.5, 0.8] 
→ 0.8짜리에 훨씬 더 큰 가중치!
→ "0.8을 만든 패턴을 강하게 학습해!"
```

#### 왜 이게 중요한가?

**발견(Discovery)의 목표:**
- ❌ 평균적으로 괜찮은 솔루션 100개
- ✅ 최고의 솔루션 **딱 1개**

그래서 최고 보상 솔루션에 집중하는 엔트로픽 목표가 적합합니다.

---

## 🖼️ 시각적으로 이해하기

```
┌────────────────────────────────────────────────────────────────┐
│                    한 스텝의 학습 과정                           │
├────────────────────────────────────────────────────────────────┤
│                                                                │
│  ┌─────────┐     512개 생성      ┌─────────────────────────┐   │
│  │ LLM θᵢ  │ ─────────────────→ │ 솔루션들 + 보상          │   │
│  │(현재)   │                     │                         │   │
│  └─────────┘                     │ 솔루션 1: 보상 0.2 😢   │   │
│       │                          │ 솔루션 2: 보상 0.3 😐   │   │
│       │                          │ 솔루션 3: 보상 0.8 🎉   │   │
│       │                          │ ...                     │   │
│       │                          └─────────────────────────┘   │
│       │                                      │                 │
│       │         ┌────────────────────────────┘                 │
│       │         │                                              │
│       │         ▼                                              │
│       │    ┌─────────────────────────────────────────┐         │
│       │    │           가중치 업데이트 규칙            │         │
│       │    │                                         │         │
│       │    │  보상 0.8 솔루션 → 이 패턴 확률 UP ⬆️⬆️  │         │
│       │    │  보상 0.3 솔루션 → 이 패턴 확률 약간 UP  │         │
│       │    │  보상 0.2 솔루션 → 이 패턴 확률 DOWN ⬇️  │         │
│       │    │                                         │         │
│       │    │  (엔트로픽 목표: 최고 보상에 집중!)       │         │
│       │    └─────────────────────────────────────────┘         │
│       │                          │                             │
│       │                          ▼                             │
│       │         θᵢ₊₁ = θᵢ + 학습률 × 그래디언트               │
│       │                          │                             │
│       ▼                          ▼                             │
│  ┌─────────┐              ┌─────────┐                          │
│  │ LLM θᵢ  │    변환     │ LLM θᵢ₊₁│                          │
│  │         │ ─────────→  │ (개선됨) │                          │
│  └─────────┘              └─────────┘                          │
│                                 │                              │
│                                 ▼                              │
│                    다음 스텝에서 더 좋은                         │
│                    솔루션 생성 확률 UP!                          │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

---

## 🔧 기술적 세부사항 (LoRA)

전체 LLM을 업데이트하면:
- 수십억 개 파라미터 → 너무 느리고 비쌈 💸

**LoRA (Low-Rank Adaptation):**
```
원래 가중치 θ는 고정!
작은 "어댑터" 부분만 학습 (rank=32)

θ_전체 = θ_원래(고정) + θ_어댑터(학습)

→ 학습할 파라미터 수 대폭 감소
→ 빠르고 저렴하게 적응 가능
```

---

## 📊 실제 효과: 분포의 변화

```
Step 0 (학습 전):
보상 분포: [대부분 낮음]
██████████████░░░░░░░░░░░░░░░░░░
      ↑ 대부분 여기

Step 25 (중간):
보상 분포: [중간으로 이동]
░░░░░░██████████████░░░░░░░░░░░
            ↑ 여기로 이동

Step 49 (학습 후):
보상 분포: [높은 쪽으로 이동!]
░░░░░░░░░░░░░░░░██████████████░
                      ↑ 최고 성능!
```

**학습이 진행될수록:**
- 나쁜 솔루션 생성 확률 ⬇️
- 좋은 솔루션 생성 확률 ⬆️

---

## 🎯 한 문장 요약

> **"좋은 결과를 낸 코드 패턴은 더 자주 생성하고, 나쁜 결과를 낸 패턴은 덜 생성하도록 LLM의 내부 숫자(가중치)를 조금씩 조정하는 것"**

이것이 바로 테스트 타임에 RL로 LLM을 개선하는 핵심 원리입니다!

아주 핵심적인 질문이에요! 이게 RL의 가장 까다로운 부분인데, 최대한 직관적으로 설명해 드릴게요.

---

## 🤔 문제의 핵심

```
LLM 출력: "def kernel(): return x * 2"  ← 이건 그냥 글자들
                                          미분이 안 됨! 😱
```

맞아요. **텍스트 자체는 미분할 수 없습니다.** 
"d" → "e" → "f" 이런 글자에 미분이 뭔 의미가 있겠어요?

그런데 비밀이 있습니다...

---

## 💡 핵심 통찰: 우리가 미분하는 건 "글자"가 아니라 "확률"

### LLM이 실제로 하는 일

LLM은 글자를 직접 내뱉는 게 아니라, **각 글자의 확률**을 계산합니다:

```
입력: "def ke"

LLM 내부 계산 결과 (확률 분포):
┌─────────────────────────────────┐
│  다음 글자가 뭘까?               │
│                                 │
│  "r" → 75% ◀── 가장 높음        │
│  "n" → 10%                      │
│  "y" → 8%                       │
│  "a" → 3%                       │
│  ...나머지 → 4%                 │
└─────────────────────────────────┘

→ 확률에 따라 랜덤 선택 → "r" 선택됨
→ 출력: "def ker"
```

**이 "확률"은 θ로부터 계산된 숫자입니다!**
**숫자는 미분 가능합니다!** ✅

---

## 🔄 전체 과정을 따라가 봅시다

### 1단계: 토큰 단위로 생성

LLM은 한 글자(토큰)씩 생성합니다:

```
"def kernel(): return x * 2"

이건 사실 이런 과정:
┌──────────────────────────────────────────────────────────┐
│ Step 1: P("def" | 시작) = 0.15  → "def" 선택            │
│ Step 2: P(" " | "def") = 0.95  → " " 선택               │
│ Step 3: P("kernel" | "def ") = 0.08 → "kernel" 선택     │
│ Step 4: P("(" | "def kernel") = 0.92 → "(" 선택         │
│ ...                                                      │
│ Step N: P("2" | "...x * ") = 0.35 → "2" 선택            │
└──────────────────────────────────────────────────────────┘
```

**각 단계에서 확률이 계산됩니다!**

---

### 2단계: 전체 문장의 확률 계산

전체 출력의 확률 = 각 토큰 확률의 곱:

```
P("def kernel(): return x * 2" | 입력)
= P("def") × P(" "|"def") × P("kernel"|"def ") × ... × P("2"|"...x * ")
= 0.15 × 0.95 × 0.08 × 0.92 × ... × 0.35
```

보통 로그를 취해서 더하기로 바꿉니다:

```
log P(전체 출력) = log P("def") + log P(" ") + log P("kernel") + ...
                = -1.9 + (-0.05) + (-2.5) + (-0.08) + ... 
                = -15.3 (예시)
```

---

### 3단계: 이제 미분이 가능!

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   입력 ──→ [LLM 가중치 θ] ──→ 확률 분포 ──→ log P(출력)    │
│              ↑                              ↓               │
│              │                              │               │
│              └──────── 미분 가능! ──────────┘               │
│                                                             │
│   ∂(log P) / ∂θ = 계산 가능한 숫자!                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 🎯 Policy Gradient: 마법의 공식

### 핵심 아이디어

```
"좋은 출력의 확률은 올리고, 나쁜 출력의 확률은 내리자"
```

### 수식 (걱정 마세요, 바로 해석해 드릴게요)

```
θ 업데이트 방향 = (보상 R) × ∇θ log P(출력 | 입력)
                    ↑              ↑
                 얼마나 좋았나    이 출력의 확률을 
                                 높이는 방향
```

### 해석

```
만약 보상 R = +10 (아주 좋음):
→ θ를 "이 출력 확률을 높이는 방향"으로 크게 이동

만약 보상 R = +0.1 (별로):
→ θ를 "이 출력 확률을 높이는 방향"으로 아주 조금만 이동
   (사실상 거의 안 바뀜)

만약 보상 R = -5 (나쁨, baseline 대비):
→ θ를 "이 출력 확률을 낮추는 방향"으로 이동!
```

---

## 📊 구체적인 예시

```
┌─────────────────────────────────────────────────────────────────┐
│                        학습 데이터                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  출력 1: "def kernel_v1(): ..."                                 │
│    → log P = -12.5                                              │
│    → 보상 = 0.2 (느림)                                          │
│    → 업데이트: 0.2 × ∇(-12.5) → 약한 양의 업데이트              │
│                                                                 │
│  출력 2: "def kernel_v2(): ..."                                 │
│    → log P = -15.3                                              │
│    → 보상 = 0.8 (빠름! 🎉)                                      │
│    → 업데이트: 0.8 × ∇(-15.3) → 강한 양의 업데이트! ⬆️⬆️       │
│                                                                 │
│  출력 3: "def kernel_v3(): ..."                                 │
│    → log P = -11.2                                              │
│    → 보상 = 0.1 (아주 느림)                                     │
│    → 업데이트: 0.1 × ∇(-11.2) → 거의 무시                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

결과: θ가 "kernel_v2 같은 출력"을 더 자주 생성하도록 바뀜!
```

---

## 🖼️ 시각적 요약

```
┌──────────────────────────────────────────────────────────────────┐
│                                                                  │
│  [프롬프트] ──→ [LLM θ] ──→ [확률 분포] ──샘플링──→ [텍스트]     │
│                   │              │                    │          │
│                   │              │                    ▼          │
│                   │              │              [환경에서 실행]   │
│                   │              │                    │          │
│                   │              │                    ▼          │
│                   │              │              [보상 R 획득]     │
│                   │              │                    │          │
│                   │              ▼                    │          │
│                   │         log P(텍스트)             │          │
│                   │              │                    │          │
│                   │              └────────┬───────────┘          │
│                   │                       │                      │
│                   │                       ▼                      │
│                   │              R × ∇θ log P                    │
│                   │                       │                      │
│                   │                       │ (역전파!)             │
│                   ◀───────────────────────┘                      │
│                                                                  │
│              θ_new = θ_old + 학습률 × (R × ∇θ log P)             │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘

핵심: 텍스트 자체가 아니라, 
      "그 텍스트를 생성할 확률"에 대해 역전파!
```

---

## 🎓 핵심 정리

| 질문 | 답변 |
|------|------|
| 텍스트에 미분이 가능한가? | ❌ 불가능 |
| 그럼 뭘 미분하나? | ✅ **텍스트 생성 확률** |
| 확률은 어디서 오나? | LLM 가중치 θ에서 계산됨 |
| 어떻게 업데이트? | 좋은 출력의 확률 ↑, 나쁜 출력의 확률 ↓ |

---

## 💬 비유로 마무리

> **강아지 훈련 비유 확장:**
> 
> 강아지가 "앉아"를 듣고 여러 행동 중 하나를 선택합니다.
> - 앉기: 30% 확률
> - 눕기: 40% 확률  
> - 뛰기: 30% 확률
> 
> 강아지가 "앉기"를 선택했고, 간식을 받았습니다!
> 
> 우리가 바꾸는 건 **강아지의 실제 행동**이 아니라,
> **강아지 뇌 속의 "앉기 선택 확률"**입니다.
> 
> 다음에는: 앉기 50%, 눕기 30%, 뛰기 20%
> 
> LLM도 똑같습니다. **출력 텍스트**가 아니라 
> **그 텍스트를 선택할 확률**을 조정하는 겁니다!